{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cody/Documents/DataSciBC/GameRFL/Pong/Deep_Q_Model\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "parentdir = pathlib.Path.cwd().parent.parent\n",
    "sys.path.append(str(parentdir))\n",
    "import Pong.Deep_Q_Model.training as trainer\n",
    "import Pong.Deep_Q_Model.agent as agent\n",
    "\n",
    "import os\n",
    "file = os.getcwd()\n",
    "print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "cumulative reward: 155.0\n",
      "1.0\n",
      "episode 1\n",
      "episode 1 with loss 0.009259074926376343\n",
      "cumulative reward: 245.0\n",
      "0.91\n",
      "episode 2\n",
      "episode 2 with loss 969959232.0\n",
      "episode 2 with loss 6190287872.0\n",
      "episode 2 with loss 914841728.0\n",
      "episode 2 with loss 1169410560.0\n",
      "episode 2 with loss 1075478272.0\n",
      "episode 2 with loss 812207616.0\n",
      "cumulative reward: 165.0\n",
      "0.8200000000000001\n",
      "episode 3\n",
      "episode 3 with loss 701473.875\n",
      "episode 3 with loss 173657824.0\n",
      "episode 3 with loss 157830464.0\n",
      "episode 3 with loss 15515422.0\n",
      "cumulative reward: 40.0\n",
      "0.73\n",
      "episode 4\n",
      "episode 4 with loss 1563318.25\n",
      "episode 4 with loss 26056464.0\n",
      "episode 4 with loss 49948304.0\n",
      "episode 4 with loss 60076656.0\n",
      "cumulative reward: 105.0\n",
      "0.64\n",
      "episode 5\n",
      "episode 5 with loss 811743.625\n",
      "episode 5 with loss 394911.625\n",
      "episode 5 with loss 987128.375\n",
      "episode 5 with loss 1572386.5\n",
      "episode 5 with loss 1837835.875\n",
      "cumulative reward: 210.0\n",
      "0.55\n",
      "episode 6\n",
      "episode 6 with loss 37446.046875\n",
      "episode 6 with loss 115700.3203125\n",
      "episode 6 with loss 293814.09375\n",
      "cumulative reward: 95.0\n",
      "0.45999999999999996\n",
      "episode 7\n",
      "episode 7 with loss 1623321.25\n",
      "episode 7 with loss 1567556.875\n",
      "episode 7 with loss 153257.71875\n",
      "episode 7 with loss 83317.6015625\n",
      "cumulative reward: 230.0\n",
      "0.37\n",
      "episode 8\n",
      "episode 8 with loss 74152.671875\n",
      "episode 8 with loss 1475971.0\n",
      "episode 8 with loss 19070.029296875\n",
      "episode 8 with loss 29390.26171875\n",
      "episode 8 with loss 49050.68359375\n",
      "episode 8 with loss 40950.84765625\n",
      "episode 8 with loss 18622.47265625\n",
      "episode 8 with loss 10724.97265625\n",
      "cumulative reward: 535.0\n",
      "0.28\n",
      "episode 9\n",
      "episode 9 with loss 12126.81640625\n",
      "episode 9 with loss 13240.666015625\n",
      "cumulative reward: 20.0\n",
      "0.19000000000000006\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RecordEpisodeStatistics' object has no attribute 'dist_rewards'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m Pong_trainer \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mPong_training(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,update_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mPong_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataSciBC/GameRFL/Deep_Q_Learning/training.py:146\u001b[0m, in \u001b[0;36mQ_training.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay_epsilon(ep,n_episodes)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m#checkpoint save:\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m#if ep %20==0:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m#self.save_checkpoint(ep)\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_rewards(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_rewards\u001b[49m)\n\u001b[1;32m    147\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RecordEpisodeStatistics' object has no attribute 'dist_rewards'"
     ]
    }
   ],
   "source": [
    "Pong_trainer = trainer.Pong_training('test',n_episodes=10,update_factor=100)\n",
    "Pong_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pong_trainer.results()\n",
    "Pong_trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pong_trainer.agent.run_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GameRFL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
